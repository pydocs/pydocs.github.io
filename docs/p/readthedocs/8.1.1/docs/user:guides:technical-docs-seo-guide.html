<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet"
          href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"
          integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf"
          crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
    <script type="text/x-mathjax-config">
      // this should process only math inside  span with tex2jax_process class
      MathJax.Hub.Config({
          extensions: ["tex2jax.js"],
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
          },
          "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js">
    </script>
    <link rel="stylesheet" href="/papyri.css">
    <link rel="stylesheet" href="/pygments.css">
<style>

    header > brand {
        /*border-right: 1px solid #ccc;
          border-left: 1px solid #ccc;*/
        margin-left: 15px;
        padding: 0px;
        display: inline-block; 
        width: 67px;
    }

    brand > a {
        text-decoration: None;

    }
    brand >a> img {
            display: inline-block;
    margin: 0;
    padding: 0;
    position: relative;
    bottom: -10px;

   }
    .container {
       max-width: 800px;
       padding-left: 210px;
       padding-top: 30px;
    }
</style>
</head><body class="tex2jax_ignore">
    <header>
        <brand><a href='/'><img src='/favicon.ico'/></a></brand>
        <nav> / <a href='/' >p</a>  &nbsp;/&nbsp; 
                <div class="dropdown">
                <a href=readthedocs>readthedocs</a>&nbsp;/&nbsp;
                    <div class="dropdown-content">
                     </div>
                </div>
                <div class="dropdown">
                <a href='/p/readthedocs/8.1.1/api/readthedocs'>8.1.1</a>&nbsp;/&nbsp;
                    <div class="dropdown-content">
<a href="/p/readthedocs/8.1.1/api/readthedocs">8.1.1</a>
                    </div>
                </div>

                <div class="dropdown">
                    <a href='/p/readthedocs/8.1.1/api/readthedocs'>docs</a>&nbsp;/&nbsp;
                    <div class="dropdown-content">
                        <a href="/p/readthedocs/8.1.1/api/readthedocs">API</a>
                        <a href="/p/readthedocs/8.1.1/gallery">Gallery</a>
                        <a href="/p/readthedocs/8.1.1/examples">Examples</a>
                        <a href="/p/readthedocs/8.1.1/docs">Narrative</a>
                     </div>
                </div>

</nav>
    </header>
    <div class='container'>

    <div class="sidenav">
        <i class="fab fa-python"></i>
        <a href="#">Project Logo ^</a>
    <a href="#">readthedocs</a>
    <a href="#">8.1.1</a>
        <a class='external' href="https://github.com/readthedocs/readthedocs.org">GitHub</a>
    <hr/>
    <hr/>
    </div><!--end sidenav-->







<h1>Technical Documentation Search Engine Optimization (SEO) Guide</h1>
               <pre class='not-implemented'>
.. meta:: 
    (&#39;description lang=en&#39;, &#39;Looking to optimize your Sphinx documentation for search engines?\n        This SEO guide will help your docs be better understood by both people and crawlers\n        as well as help you rank higher in search engine results.&#39;)
    </pre>


               <p>This guide will help you optimize your documentation for search engines with the goal of increasing traffic to your docs. While you optimize your docs to make them more crawler friendly for search engine spiders, it&#39;s important to keep in mind that your ultimate goal is to make your docs more discoverable for your users. <strong>You&#39;re trying to make sure that when a user types a question into a search engine
that is answerable by your documentation, that your docs appear in the results.</strong></p>

               <p>This guide isn&#39;t meant to be your only resource on SEO, and there&#39;s a lot of SEO topics not covered here. For additional reading, please see the         <code class='verbatim'>external resources &lt;guides/technical-docs-seo-guide:External resources&gt;</code>
 section.</p>

               <p>While many of the topics here apply to all forms of technical documentation, this guide will focus on Sphinx, which is the most common documentation authoring tool on Read the Docs, as well as improvements provided by Read the Docs itself.</p>

               <pre class='not-implemented'>
.. contents:: Table of contents
    (&#39;local&#39;, &#39;&#39;)
    (&#39;backlinks&#39;, &#39;none&#39;)
    (&#39;depth&#39;, &#39;3&#39;)
    </pre>



<h2>SEO Basics</h2>
               <p>Search engines like Google and Bing crawl through the internet following links in an attempt to understand and build an index of what various pages and sites are about. This is called &#34;crawling&#34; or &#34;indexing&#34;. When a person sends a query to a search engine, the search engine evaluates this index using a number of factors and attempts to return the results most likely to answer that person&#39;s question.</p>

               <p>How search engines &#34;rank&#34; sites based on a person&#39;s query is part of their secret sauce. While some search engines publish the basics of their algorithms (see Google&#39;s published details on PageRank), few search engines give all of the details in an attempt to prevent users from gaming the rankings with low value content which happens to rank well.</p>

               <p>Both <a link href="https://support.google.com/webmasters/answer/35769" class='external'>Google</a> and <a link href="https://www.bing.com/webmaster/help/webmaster-guidelines-30fba23a" class='external'>Bing</a> publish a set of guidelines to help make sites easier to understand for search engines and rank better. To summarize some of the most important aspects as they apply to technical documentation, your site should:</p>

                          <ul>               <li>               <p>Use descriptive and accurate titles in the HTML         <code class='verbatim'>&lt;title&gt;</code>
 tag.   For Sphinx, the         <code class='verbatim'>&lt;title&gt;</code>
 comes from the first heading on the page.</p>

</li>
               <li>               <p>Ensure your URLs are descriptive. They are displayed in search results.   Sphinx uses the source filename without the file extension as the URL.</p>

</li>
               <li>               <p>Make sure the words your readers would search for to find your site   are actually included on your pages.</p>

</li>
               <li>               <p>Avoid low content pages or pages with very little original content.</p>

</li>
               <li>               <p>Avoid tactics that attempt to increase your search engine ranking   without actually improving content.</p>

</li>
               <li>               <p>Google specifically <code class='not-implemented'>:None:None:`warns about automatically generated content`</code>   although this applies primarily to keyword stuffing and low value content.   High quality documentation generated from source code   (eg. auto generated API documentation) seems OK.</p>

                          <pre class='not-implemented'>
            &lt;Unimplemented &#39;target&#39; &#39;.. _warns about automatically generated content: https://support.google.com/webmasters/answer/2721306&#39;&gt;
           </pre>

</li>
            </ul>

               <p>While both Google and Bing discuss site performance as an important factor in search result ranking, this guide is not going to discuss it in detail. Most technical documentation that uses Sphinx or Read the Docs generates static HTML and the performance is typically decent relative to most of the internet.</p>


<h2>Optimizing your docs for search engine spiders</h2>
               <p>Once a crawler or spider finds your site, it will follow links and redirects in an attempt to find any and all pages on your site. While there are a few ways to guide the search engine in its crawl for example by using a         <code class='verbatim'>sitemap &lt;guides/technical-docs-seo-guide:Use a sitemap.xml file&gt;</code>
 or a         <code class='verbatim'>robots.txt file &lt;guides/technical-docs-seo-guide:Use a robots.txt file&gt;</code>
 which we&#39;ll discuss shortly, the most important thing is making sure the spider can follow links on your site and get to all your pages.</p>


<h3>Avoid orphan pages</h3>
               <p>Sphinx calls pages that don&#39;t have links to them &#34;orphans&#34; and will throw a warning while building documentation that contains an orphan unless the warning is silenced with the         <code class='verbatim'>orphan directive &lt;sphinx:metadata&gt;</code>
:</p>

               <pre>$ make html
sphinx-build -b html -d _build/doctrees . _build/html
Running Sphinx v1.8.5
...
checking consistency... /path/to/file.rst: WARNING: document isn&#39;t included in any toctree
done
...
build finished with problems, 1 warning.</pre>
               <p>You can make all Sphinx warnings into errors during your build process by setting         <code class='verbatim'>SPHINXOPTS = -W --keep-going</code>
 in your Sphinx Makefile.</p>


<h3>Avoid uncrawlable content</h3>
               <p>While typically this isn&#39;t a problem with technical documentation, try to avoid content that is &#34;hidden&#34; from search engines. This includes content hidden in images or videos which the crawler may not understand. For example, if you do have a video in your docs, make sure the rest of that page describes the content of the video.</p>

               <p>When using images, make sure to set the image alt text or set a caption on figures. For Sphinx, the image and figure directives support this:</p>

               <pre class='not-implemented'>
.. sourcecode:: rst
    .. image:: your-image.png
        :alt: A description of this image

    .. figure:: your-image.png

        A caption for this figure</pre>



<h3>Redirects</h3>
               <p>Redirects tell search engines when content has moved. For example, if this guide moved from         <code class='verbatim'>guides/technical-docs-seo-guide.html</code>
 to         <code class='verbatim'>guides/sphinx-seo-guide.html</code>
, there will be a time period where search engines will still have the old URL in their index and will still be showing it to users. This is why it is important to update your own links within your docs as well as redirecting. If the hostname moved from docs.readthedocs.io to docs.readthedocs.org, this would be even more important!</p>

               <p>Read the Docs supports a few different kinds of         <code class='verbatim'>user defined redirects &lt;/user-defined-redirects&gt;</code>
 that should cover all the different cases such as redirecting a certain page for all project versions, or redirecting one version to another.</p>


<h3>Canonical URLs</h3>
               <p>Anytime very similar content is hosted at multiple URLs, it is pretty important to set a canonical URL. The canonical URL tells search engines where the original version your documentation is even if you have multiple versions on the internet (for example, incomplete translations or deprecated versions).</p>

               <p>Read the Docs supports         <code class='verbatim'>setting the canonical URL &lt;custom_domains:Canonical URLs&gt;</code>
 if you are using a         <code class='verbatim'>custom domain &lt;/custom_domains&gt;</code>
 under <code class='not-implemented'>:None:guilabel:`Admin`</code> &gt; <code class='not-implemented'>:None:guilabel:`Domains`</code> in the Read the Docs dashboard.</p>


<h3>Use a robots.txt file</h3>
               <p>A         <code class='verbatim'>robots.txt</code>
 file is readable by crawlers and lives at the root of your site (eg. https://docs.readthedocs.io/robots.txt). It tells search engines which pages to crawl or not to crawl and can allow you to control how a search engine crawls your site. For example, you may want to request that search engines         <code class='verbatim'>ignore unsupported versions of your documentation &lt;faq:How can I avoid search results having a deprecated version of my docs?&gt;</code>
 while keeping those docs online in case people need them.</p>

               <p>By default, Read the Docs serves a         <code class='verbatim'>robots.txt</code>
 for you. To customize this file, you can create a         <code class='verbatim'>robots.txt</code>
 file that is written to your documentation root on your default branch/version.</p>

               <p>See the <a link href="https://support.google.com/webmasters/answer/6062608" class='external'>Google&#39;s documentation on robots.txt</a> for additional details.</p>


<h3>Use a sitemap.xml file</h3>
               <p>A sitemap is a file readable by crawlers that contains a list of pages and other files on your site and some metadata or relationships about them (eg. https://docs.readthedocs.io/sitemap.xml). A good sitemaps provides information like how frequently a page or file is updated or any alternate language versions of a page.</p>

               <p>Read the Docs generates a sitemap for you that contains the last time your documentation was updated as well as links to active versions, subprojects, and translations your project has. We have a small separate guide on         <code class='verbatim'>sitemaps &lt;hosting:Sitemaps&gt;</code>
.</p>

               <p>See the <a link href="https://support.google.com/webmasters/answer/183668" class='external'>Google docs on building a sitemap</a>.</p>


<h3>Use meta tags</h3>
               <p>Using a meta description allows you to customize how your pages look in search engine result pages.</p>

               <p>Typically search engines will use the first few sentences of a page if no meta description is provided. In Sphinx, you can customize your meta description using the following RestructuredText:</p>

               <pre class='not-implemented'>
.. sourcecode:: rst
    .. meta::
        :description lang=en:
            Adding additional CSS or JavaScript files to your Sphinx documentation
            can let you customize the look and feel of your docs or add additional functionality.</pre>


               <pre class='not-implemented'>
.. figure:: ../_static/images/guides/google-search-engine-results.png
    (&#39;align&#39;, &#39;center&#39;)
    (&#39;figwidth&#39;, &#39;80%&#39;)
    Google search engine results showing a customized meta description</pre>


               <p>Moz.com, an authority on search engine optimization, makes the following suggestions for meta descriptions:</p>

                          <ul>               <li>               <p>Your meta description should have the most relevant content of the page.   A searcher should know whether they&#39;ve found the right page from the description.</p>

</li>
               <li>               <p>The meta description should be between 150-300 characters   and it may be truncated down to around 150 characters in some situations.</p>

</li>
               <li>               <p>Meta descriptions are used for display but not for ranking.</p>

</li>
            </ul>

               <p>Search engines don&#39;t always use your customized meta description if they think a snippet from the page is a better description.</p>


<h2>Measure, iterate, &amp; improve</h2>
               <p>Search engines (and soon, Read the Docs itself) can provide useful data that you can use to improve your docs&#39; ranking on search engines.</p>


<h3>Search engine feedback</h3>
               <p><a link href="https://search.google.com/search-console" class='external'>Google Search Console</a> and <a link href="https://www.bing.com/webmaster/help/webmaster-guidelines-30fba23a" class='external'>Bing Webmaster Tools</a> are tools for webmasters to get feedback about the crawling of their sites (or docs in our case). Some of the most valuable feedback these provide include:</p>

                          <ul>               <li>               <p>Google and Bing will show pages that were previously indexed that now give a 404   (or more rarely a 500 or other status code).   These will remain in the index for some time but will eventually be removed.   This is a good opportunity to create a         <code class='verbatim'>redirect &lt;guides/technical-docs-seo-guide:Redirects&gt;</code>
.</p>

</li>
               <li>               <p>These tools will show any crawl issues with your documentation.</p>

</li>
               <li>               <p>Search Console and Webmaster Tools will highlight security issues found   or if Google or Bing took action against your site because they believe it is spammy.</p>

</li>
            </ul>


<h3>Analytics tools</h3>
               <p>A tool like         <code class='verbatim'>Google Analytics &lt;analytics:Enabling Google Analytics on your Project&gt;</code>
 can give you feedback about the search terms people use to find your docs, your most popular pages, and lots of other useful data.</p>

               <p>Search term feedback can be used to help you optimize content for certain keywords or for related keywords. For Sphinx documentation, or other technical documentation that has its own search features, analytics tools can also tell you the terms people search for within your site.</p>

               <p>Knowing your popular pages can help you prioritize where to spend your SEO efforts. Optimizing your already popular pages can have a significant impact.</p>


<h2>External resources</h2>
               <p>Here are a few additional resources to help you learn more about SEO and rank better with search engines.</p>

                          <ul>               <li>               <p><a link href="https://moz.com/beginners-guide-to-seo" class='external'>Moz&#39;s beginners guide to SEO</a></p>

</li>
               <li>               <p><a link href="https://support.google.com/webmasters/answer/35769" class='external'>Google&#39;s Webmaster Guidelines</a></p>

</li>
               <li>               <p><a link href="https://www.bing.com/webmaster/help/webmaster-guidelines-30fba23a" class='external'>Bing&#39;s Webmaster Guidelines</a></p>

</li>
               <li>               <p><a link href="https://support.google.com/webmasters/answer/7451184" class='external'>Google&#39;s SEO Starter Guide</a></p>

</li>
            </ul>


<h3 id="section-Examples">Examples</h3>
         See :
    


<h3>Local connectivity graph</h3>
<p>Hover to see nodes names; edges to Self not shown, Caped at 50 nodes.</p>

<p> Using a canvas is more power efficient and can get hundred of nodes ; but does not allow hyperlinks; , arrows
or text (beyond on hover) </p> 
<canvas class='graph' width="800" height="500"></canvas>
<p> SVG is more flexible but power hungry; and does not scale well to 50 + nodes. </p> 

<svg class='graph' width="600" height="500"></svg>

<p>All aboves nodes referred to, (or are referred from) current nodes; Edges from Self to other have been omitted
(or all nodes would be connected to the central node "self" which is not useful). Nodes are colored by the library
they belong to, and scaled with the number of references pointing them</p>

<hr>

    GitHub : <a class='external' href='https://github.com/readthedocs/readthedocs.org/blob//None#LNone'>None#None</a>
<br/>

type: None <br/>
Commit: <br/>

        <script src="https://d3js.org/d3.v4.min.js"></script>
    <script src="https://d3js.org/d3-selection-multi.v1.js"></script>
    <style type="text/css">
        .node {};
        .link { stroke: #999; stroke-opacity: .6; stroke-width: 1px; };
    </style>
<script>
    window._data_graph = {};
</script>
<script type="text/javascript" src='/graph_canvas.js'></script>
<script type="text/javascript" src='/graph_svg.js'></script>


</div>
</body>
</html>