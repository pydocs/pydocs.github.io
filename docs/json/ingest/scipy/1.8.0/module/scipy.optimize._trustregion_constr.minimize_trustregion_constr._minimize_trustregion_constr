Ùª­jAttributesÙ¯‚€öpExtended SummaryÙ¯‚€ögMethodsÙ¯‚€öeNotesÙ¯‚€öpOther ParametersÙ¯‚€öjParametersÙ¯‚ŒÙ°ƒdgtolofloat, optionalÙ¹‚ƒÙ§xÚTolerance for termination by the norm of the Lagrangian gradient. The algorithm will terminate when both the infinity norm (i.e., max abs value) of the Lagrangian gradient and the constraint violation are smaller than Ù¡dgtolÙ§r. Default is 1e-8.€Ù°ƒdxtolofloat, optionalÙ¹‚…Ù§xgTolerance for termination by the change of the independent variable. The algorithm will terminate when Ù¡ptr_radius < xtolÙ§h, where Ù¡itr_radiusÙ§xJ is the radius of the trust region used in the algorithm. Default is 1e-8.€Ù°ƒkbarrier_tolofloat, optionalÙ¹‚ƒÙ§xµThreshold on the barrier parameter for the algorithm termination. When inequality constraints are present, the algorithm will terminate only when the barrier parameter is less than Ù¢„kbarrier_tolÙ „ööelocalkbarrier_tolelocalõÙ§r. Default is 1e-8.€Ù°ƒosparse_jacobianv{bool, None}, optionalÙ¹‚Ù§yDetermines how to represent Jacobians of the constraints. If bool, then Jacobians of all the constraints will be converted to the corresponding format. If None (default), then Jacobians won't be converted, but the algorithm can proceed only if they all have the same format.€Ù°ƒx"initial_tr_radius: float, optional`Ù¹‚ƒÙ§yÉInitial trust radius. The trust radius gives the maximum distance between solution points in consecutive iterations. It reflects the trust the algorithm puts in the local approximation of the optimization problem. For an accurate local approximation the trust-region should be large and for an  approximation valid only close to the current point it should be a small one. The trust radius is automatically updated throughout the optimization process, with Ù¡qinitial_tr_radiusÙ§x@ being its initial value. Default is 1 (recommended in , p. 19).€Ù°ƒvinitial_constr_penaltyofloat, optionalÙ¹‚‰Ù§xÕInitial constraints penalty parameter. The penalty parameter is used for balancing the requirements of decreasing the objective function and satisfying the constraints. It is used for defining the merit function: Ù¡x?merit_function(x) = fun(x) + constr_penalty * constr_norm_l2(x)Ù§h, where Ù¡qconstr_norm_l2(x)Ù§xƒ is the l2 norm of a vector containing all the constraints. The merit function is used for accepting or rejecting trial points and Ù¡nconstr_penaltyÙ§x£ weights the two conflicting goals of reducing objective function and constraints. The penalty is automatically updated throughout the optimization  process, with Ù¡vinitial_constr_penaltyÙ§x@ being its  initial value. Default is 1 (recommended in , p 19).€Ù°ƒxEinitial_barrier_parameter, initial_barrier_tolerance: float, optional`Ù¹‚“Ù§x¯Initial barrier parameter and initial tolerance for the barrier subproblem. Both are used only when inequality constraints are present. For dealing with optimization problems Ù¡jmin_x f(x)Ù§x# subject to inequality constraints Ù¡ic(x) <= 0Ù§x? the algorithm introduces slack variables, solving the problem Ù¡x-min_(x,s) f(x) + barrier_parameter*sum(ln(s))Ù§x& subject to the equality constraints  Ù¡lc(x) + s = 0Ù§xU instead of the original problem. This subproblem is solved for decreasing values of Ù¡qbarrier_parameterÙ§xC and with decreasing tolerances for the termination, starting with Ù¡xinitial_barrier_parameterÙ§x for the barrier parameter and Ù¡xinitial_barrier_toleranceÙ§xc for the barrier tolerance. Default is 0.1 for both values (recommended in  p. 19). Also note that Ù¡qbarrier_parameterÙ§e and Ù¡qbarrier_toleranceÙ§x% are updated with the same prefactor.€Ù°ƒtfactorization_methodxstring or None, optional„Ù¹‚Ù§xiMethod to factorize the Jacobian of the constraints. Use None (default) for the auto selection or one of:€Ù·„x+- 'NormalEquation' (requires scikit-sparse)s- 'AugmentedSystem's- 'QRFactorization't- 'SVDFactorization'Ù¹‚ƒÙ§y1The methods 'NormalEquation' and 'AugmentedSystem' can be used only with sparse constraints. The projections required by the algorithm will be computed using, respectively, the the normal equation  and the augmented system approaches explained in . 'NormalEquation' computes the Cholesky factorization of Ù¡eA A.TÙ§x¬ and 'AugmentedSystem' performs the LU factorization of an augmented system. They usually provide similar results. 'AugmentedSystem' is used by default for sparse matrices.€Ù¹‚Ù§yÓThe methods 'QRFactorization' and 'SVDFactorization' can be used only with dense constraints. They compute the required projections using, respectively, QR and SVD factorizations. The 'SVDFactorization' method can cope with Jacobian matrices with deficient row rank and will be used whenever other factorization methods fail (which may imply the conversion of sparse matrices to a dense format when required). By default, 'QRFactorization' is used for dense matrices.€Ù°ƒtfinite_diff_rel_stepxNone or array_like, optionalÙ¹‚Ù§x;Relative step size for the finite difference approximation.€Ù°ƒgmaxitermint, optionalÙ¹‚Ù§x8Maximum number of algorithm iterations. Default is 1000.€Ù°ƒgverboses{0, 1, 2}, optionalÙ¹‚Ù§xLevel of algorithm's verbosity:€Ù°ƒddispnbool, optionalÙ¹‚ƒÙ§xIf True (default), then Ù¢„gverboseÙ „escipye1.8.0fmodulex0scipy.optimize._linprog.linprog_verbose_callbackfmoduleõÙ§x will be set to 1 if it was 0.€öfRaisesÙ¯‚€öhReceivesÙ¯‚€ögReturnsÙ¯‚˜Ù°ƒ`xF`OptimizeResult` with the fields documented below. Note the following:ÙÇ‚Ù¹‚…Ù§xzAll values corresponding to the constraints are ordered as they    were passed to the solver. And values corresponding to Ù£ƒfboundsööÙ§x    constraints are put Ù¨Ù§eafterÙ§s other constraints.€Ù¹‚Ù§yrAll numbers of function, Jacobian or Hessian evaluations correspond    to numbers of actual Python function calls. It means, for example,    that if a Jacobian is estimated by finite differences, then the    number of Jacobian evaluations will be zero and the number of    function evaluations will be incremented by all calls during the    finite difference estimation.€Ù°ƒaxsndarray, shape (n,)Ù¹‚Ù§oSolution found.€Ù°ƒjoptimalityefloatÙ¹‚Ù§x9Infinity norm of the Lagrangian gradient at the solution.€Ù°ƒpconstr_violationefloatÙ¹‚Ù§x-Maximum constraint violation at the solution.€Ù°ƒcfunefloatÙ¹‚Ù§x#Objective function at the solution.€Ù°ƒdgradsndarray, shape (n,)Ù¹‚Ù§x3Gradient of the objective function at the solution.€Ù°ƒolagrangian_gradsndarray, shape (n,)Ù¹‚Ù§x4Gradient of the Lagrangian function at the solution.€Ù°ƒcnitcintÙ¹‚Ù§xTotal number of iterations.€Ù°ƒdnfevgintegerÙ¹‚Ù§x-Number of the objective function evaluations.€Ù°ƒdnjevgintegerÙ¹‚Ù§x6Number of the objective function gradient evaluations.€Ù°ƒdnhevgintegerÙ¹‚Ù§x5Number of the objective function Hessian evaluations.€Ù°ƒhcg_nitercintÙ¹‚Ù§x9Total number of the conjugate gradient method iterations.€Ù°ƒfmethodx1{'equality_constrained_sqp', 'tr_interior_point'}Ù¹‚Ù§xOptimization method used.€Ù°ƒfconstrolist of ndarrayÙ¹‚Ù§x*List of constraint values at the solution.€Ù°ƒcjacx list of {ndarray, sparse matrix}Ù¹‚Ù§xAList of the Jacobian matrices of the constraints at the solution.€Ù°ƒavolist of ndarrayÙ¹‚Ù§yList of the Lagrange multipliers for the constraints at the solution. For an inequality constraint a positive multiplier means that the upper bound is active, a negative multiplier means that the lower bound is active and if a multiplier is zero it means the constraint is not active.€Ù°ƒkconstr_nfevklist of intÙ¹‚Ù§x=Number of constraint evaluations for each of the constraints.€Ù°ƒkconstr_njevklist of intÙ¹‚Ù§xBNumber of Jacobian matrix evaluations for each of the constraints.€Ù°ƒkconstr_nhevklist of intÙ¹‚Ù§x:Number of Hessian evaluations for each of the constraints.€Ù°ƒitr_radiusefloatÙ¹‚Ù§x1Radius of the trust region at the last iteration.€Ù°ƒnconstr_penaltyefloatÙ¹‚ƒÙ§x-Penalty parameter at the last iteration, see Ù¢„vinitial_constr_penaltyÙ „ööelocalvinitial_constr_penaltyelocalõÙ§a.€Ù°ƒqbarrier_toleranceefloatÙ¹‚Ù§xjTolerance for the barrier subproblem at the last iteration. Only for problems with inequality constraints.€Ù°ƒqbarrier_parameterefloatÙ¹‚Ù§xWBarrier parameter at the last iteration. Only for problems with inequality constraints.€Ù°ƒnexecution_timeefloatÙ¹‚Ù§uTotal execution time.€Ù°ƒgmessagecstrÙ¹‚Ù§tTermination message.€Ù°ƒfstatusl{0, 1, 2, 3}Ù¹‚Ù§sTermination status:€Ù°ƒlcg_stop_condcintÙ¹‚Ù§x;Reason for CG subproblem termination at the last iteration:€ögSummaryÙ¯‚Ù¹‚Ù§x2Minimize a scalar function subject to constraints.€öhWarningsÙ¯‚€öeWarnsÙ¯‚€öfYieldsÙ¯‚€ö„gSummaryjParametersgReturnsjReferencesxB/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.pyrr<class 'function'>xKscipy.signal._filter_design.optimize._minimize._minimize_trustregion_constrÙ¯‚€ö€e1.8.0Ù«yo_minimize_trustregion_constr(fun, x0, args, grad, hess, hessp, bounds, constraints, xtol=1e-08, gtol=1e-08, barrier_tol=1e-08, sparse_jacobian=None, callback=None, maxiter=1000, verbose=0, finite_diff_rel_step=None, initial_constr_penalty=1.0, initial_tr_radius=1.0, initial_barrier_parameter=0.1, initial_barrier_tolerance=0.1, factorization_method=None, disp=False)öx[scipy.optimize._trustregion_constr.minimize_trustregion_constr._minimize_trustregion_constr€