Ùª­jAttributesÙ¯‚€öpExtended SummaryÙ¯‚€ögMethodsÙ¯‚€öeNotesÙ¯‚€öpOther ParametersÙ¯‚€öjParametersÙ¯‚‡Ù°ƒddispdboolÙ¹‚Ù§x*Set to True to print convergence messages.€Ù°ƒgmaxitercintÙ¹‚Ù§x(Maximum number of iterations to perform.€Ù°ƒdgtolefloatÙ¹‚ƒÙ§x Gradient norm must be less than Ù¢„dgtolÙ „ööelocaldgtolelocalõÙ§x before successful termination.€Ù°ƒdnormefloatÙ¹‚Ù§x(Order of norm (Inf is max, -Inf is min).€Ù°ƒcepspfloat or ndarrayÙ¹‚ƒÙ§cIf Ù£ƒkjac is NoneööÙ§xa the absolute step size used for numerical approximation of the jacobian via forward differences.€Ù°ƒjreturn_allnbool, optionalÙ¹‚Ù§xLSet to True to return a list of the best solution at each of the iterations.€Ù°ƒtfinite_diff_rel_stepxNone or array_like, optionalÙ¹‚‰Ù§cIf Ù£ƒx#jac in ['2-point', '3-point', 'cs']ööÙ§xr the relative step size to use for numerical approximation of the jacobian. The absolute step size is computed as Ù¡x)h = rel_step * sign(x0) * max(1, abs(x0))Ù§x0, possibly adjusted to fit into the bounds. For Ù¡pmethod='3-point'Ù§m the sign of Ù£ƒahööÙ§xC is ignored. If None (default) then step is selected automatically.€öfRaisesÙ¯‚€öhReceivesÙ¯‚€ögReturnsÙ¯‚€ögSummaryÙ¯‚Ù¹‚Ù§x`Minimization of scalar function of one or more variables using the conjugate gradient algorithm.€öhWarningsÙ¯‚€öeWarnsÙ¯‚€öfYieldsÙ¯‚€ö‚gSummaryjParametersx/scipy/optimize/_optimize.py r<class 'function'>x;scipy.signal._filter_design.optimize._minimize._minimize_cgÙ¯‚€ö€e1.8.0Ù«xÃ_minimize_cg(fun, x0, args=(), jac=None, callback=None, gtol=1e-05, norm=inf, eps=1.4901161193847656e-08, maxiter=None, disp=False, return_all=False, finite_diff_rel_step=None, **unknown_options)öx%scipy.optimize._optimize._minimize_cg€