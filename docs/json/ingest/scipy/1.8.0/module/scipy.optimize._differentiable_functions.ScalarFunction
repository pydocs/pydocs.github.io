Ùª­jAttributesÙ¯‚€öpExtended SummaryÙ¯‚Ù¹‚Ù§x{This class defines a scalar function F: R^n->R and methods for computing or approximating its first and second derivatives.€ögMethodsÙ¯‚€öeNotesÙ¯‚‚Ù¹‚‹Ù§x=This class implements a memoization logic. There are methods Ù¢„cfunÙ „ööelocalcfunelocalõÙ§b, Ù¢„dgradÙ „escipye1.8.0fmodulex<scipy.optimize._differentiable_functions.ScalarFunction.gradfmoduleõÙ§x%, hess` and corresponding attributes Ù¢„afÙ „escipye1.8.0fmodulex8scipy.optimize._bglu_dense._consider_refactor.<locals>.ffmoduleõÙ§b, Ù£ƒagööÙ§e and Ù£ƒaHööÙ§x,. The following things should be considered:€Ù·„x41. Use only public methods `fun`, `grad` and `hess`.xB2. After one of the methods is called, the corresponding attributexD   will be set. However, a subsequent call with a different argumentx7   of *any* of the methods may overwrite the attribute.öpOther ParametersÙ¯‚€öjParametersÙ¯‚ˆÙ°ƒcfunhcallableÙ¹‚‡Ù§x3evaluates the scalar function. Must be of the form Ù¡mfun(x, *args)Ù§h, where Ù¡axÙ§x0 is the argument in the form of a 1-D array and Ù¡dargsÙ§xq is a tuple of any additional fixed parameters needed to completely specify the function. Should return a scalar.€Ù°ƒbx0jarray-likeÙ¹‚Ù§xProvides an initial set of variables for evaluating fun. Array of real elements of size (n,), where 'n' is the number of independent variables.€Ù°ƒdargsotuple, optionalÙ¹‚Ù§xQAny additional fixed parameters needed to completely specify the scalar function.€Ù°ƒdgradx&{callable, '2-point', '3-point', 'cs'}ƒÙ¹‚Ù§xxMethod for computing the gradient vector. If it is a callable, it should be a function that returns the gradient vector:€Ù·x,``grad(x, *args) -> array_like, shape (n,)``Ù¹‚‡Ù§fwhere Ù¡axÙ§x! is an array with shape (n,) and Ù¡dargsÙ§y	 is a tuple with the fixed parameters. Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used to select a finite difference scheme for numerical estimation of the gradient with a relative step size. These finite difference schemes obey any specified Ù£ƒfboundsööÙ§a.€Ù°ƒdhessx={callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}ƒÙ¹‚Ù§xaMethod for computing the Hessian matrix. If it is callable, it should return the  Hessian matrix:€Ù·x?``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``Ù¹‚…Ù§xwhere x is a (n,) ndarray and Ù¢„dargsÙ „ööelocaldargselocalõÙ§xµ is a tuple with the fixed parameters. Alternatively, the keywords {'2-point', '3-point', 'cs'} select a finite difference scheme for numerical estimation. Or, objects implementing Ù¢„uHessianUpdateStrategyÙ „escipye1.8.0fmodulex=scipy.optimize._hessian_update_strategy.HessianUpdateStrategyfmoduleõÙ§xû interface can be used to approximate the Hessian. Whenever the gradient is estimated via finite-differences, the Hessian cannot be estimated with options {'2-point', '3-point', 'cs'} and needs to be estimated using one of the quasi-Newton strategies.€Ù°ƒtfinite_diff_rel_steprNone or array_likeÙ¹‚‡Ù§xARelative step size to use. The absolute step size is computed as Ù¡x5h = finite_diff_rel_step * sign(x0) * max(1, abs(x0))Ù§x0, possibly adjusted to fit into the bounds. For Ù¡pmethod='3-point'Ù§m the sign of Ù£ƒahööÙ§xI is ignored. If None then finite_diff_rel_step is selected automatically,€Ù°ƒrfinite_diff_boundsstuple of array_likeÙ¹‚ƒÙ§x}Lower and upper bounds on independent variables. Defaults to no bounds, (-np.inf, np.inf). Each bound must match the size of Ù¢„bx0Ù „ööelocalbx0elocalõÙ§xƒ or be a scalar, in the latter case the bound will be the same for all variables. Use it to limit the range of function evaluation.€Ù°ƒgepsilonxNone or array_like, optionalÙ¹‚‡Ù§xIAbsolute step size to use, possibly adjusted to fit into the bounds. For Ù¡pmethod='3-point'Ù§m the sign of Ù¢„gepsilonÙ „ööelocalgepsilonelocalõÙ§x9 is ignored. By default relative steps are used, only if Ù¡sepsilon is not NoneÙ§x are absolute steps used.€öfRaisesÙ¯‚€öhReceivesÙ¯‚€ögReturnsÙ¯‚€ögSummaryÙ¯‚Ù¹‚Ù§x$Scalar function and its derivatives.€öhWarningsÙ¯‚€öeWarnsÙ¯‚€öfYieldsÙ¯‚€ö„gSummarypExtended SummaryjParameterseNotesx,/scipy/optimize/_differentiable_functions.pyn<class 'type'>xMscipy.signal._filter_design.optimize._differentiable_functions.ScalarFunctionÙ¯‚€ö€e1.8.0Ù«ööx7scipy.optimize._differentiable_functions.ScalarFunction€