Ùª­jAttributesÙ¯‚€öpExtended SummaryÙ¯‚Ù¹‚…Ù§x(This function writes the dataframe as a ÙË‚lparquet filexhttps://parquet.apache.org/Ù§xU. You can choose different parquet backends, and have the option of compression. See Ù£ƒxthe user guide <io.parquet>öcrefÙ§r for more details.€ögMethodsÙ¯‚€öeNotesÙ¯‚Ù¹‚…Ù§x"This function requires either the ÙË‚kfastparquetx$https://pypi.org/project/fastparquetÙ§d or ÙË‚gpyarrowx%https://arrow.apache.org/docs/python/Ù§i library.€öpOther ParametersÙ¯‚€öjParametersÙ¯‚‡Ù°ƒdpathx9str, path object, file-like object, or None, default NoneƒÙ¹‚…Ù§x"String, path object (implementing Ù¡pos.PathLike[str]Ù§x-), or file-like object implementing a binary Ù¡gwrite()Ù§x” function. If None, the result is returned as bytes. If a string or path, it will be used as Root Directory path when writing a partitioned dataset.€ÙÆƒnversionchangeddTODOÙ¹‚Ù§f1.2.0 €Ù¹‚Ù§xPreviously this was "fname"€Ù°ƒfenginex2{'auto', 'pyarrow', 'fastparquet'}, default 'auto'Ù¹‚…Ù§x3Parquet library to use. If 'auto', then the option Ù¡qio.parquet.engineÙ§v is used. The default Ù¡qio.parquet.engineÙ§xY behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable.€Ù°ƒkcompressionx4{'snappy', 'gzip', 'brotli', None}, default 'snappy'Ù¹‚ƒÙ§x$Name of the compression to use. Use Ù¡dNoneÙ§t for no compression.€Ù°ƒeindexrbool, default NoneÙ¹‚‰Ù§cIf Ù¡dTrueÙ§x;, include the dataframe's index(es) in the file output. If Ù¡eFalseÙ§x+, they will not be written to the file. If Ù¡dNoneÙ§m, similar to Ù¡dTrueÙ§xû the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output.€Ù°ƒnpartition_colsxlist, optional, default NoneÙ¹‚Ù§xŠColumn names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string.€Ù°ƒostorage_optionsndict, optional‚Ù¹‚‰Ù§x£Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to Ù¡furllibÙ§xs as header options. For other URLs (e.g. starting with "s3://", and "gcs://") the key-value pairs are forwarded to Ù¡ffsspecÙ§m. Please see Ù¡ffsspecÙ§e and Ù¡furllibÙ§r for more details.€ÙÆƒlversionaddeddTODOÙ¹‚Ù§f1.2.0 €Ù°ƒh**kwargs`Ù¹‚ƒÙ§x8Additional arguments passed to the parquet library. See Ù£ƒvpandas io <io.parquet>öcrefÙ§r for more details.€öfRaisesÙ¯‚€öhReceivesÙ¯‚€ögReturnsÙ¯‚Ù°ƒ`x/bytes if no path argument is provided else None€ögSummaryÙ¯‚Ù¹‚Ù§x/Write a DataFrame to the binary parquet format.€öhWarningsÙ¯‚€öeWarnsÙ¯‚€öfYieldsÙ¯‚€ö‡gSummarypExtended SummaryjParametersgReturnshSee AlsoeNoteshExamplesu/pandas/core/frame.py
±r<class 'function'>xpandas.DataFrame.to_parquetÙ¯‚„Ù´ƒ˜CÙ±‚`bdfÙ±‚`a Ù±‚aoa=Ù±‚`a Ù±‚`bpdÙ±‚aoa.Ù±‚`Ù¢„iDataFrameÙ „fpandase1.4.1fmodulexpandas.core.frame.DataFramefmoduleõÙ±‚`a(Ù±‚`ddataÙ±‚aoa=Ù±‚`a{Ù±‚bs1a'Ù±‚bs1dcol1Ù±‚bs1a'Ù±‚`a:Ù±‚`a Ù±‚`a[Ù±‚bmia1Ù±‚`a,Ù±‚`a Ù±‚bmia2Ù±‚`a]Ù±‚`a,Ù±‚`a Ù±‚bs1a'Ù±‚bs1dcol2Ù±‚bs1a'Ù±‚`a:Ù±‚`a Ù±‚`a[Ù±‚bmia3Ù±‚`a,Ù±‚`a Ù±‚bmia4Ù±‚`a]Ù±‚`a}Ù±‚`a)Ù±‚`a
Ù±‚`bdfÙ±‚aoa.Ù±‚`Ù¢„jto_parquetÙ „fpandase1.4.1fmodulex&pandas.core.frame.DataFrame.to_parquetfmoduleõÙ±‚`a(Ù±‚bs1a'Ù±‚bs1odf.parquet.gzipÙ±‚bs1a'Ù±‚`a,Ù±‚`a
Ù±‚`n              Ù±‚`kcompressionÙ±‚aoa=Ù±‚bs1a'Ù±‚bs1dgzipÙ±‚bs1a'Ù±‚`a)Ù±‚`b  Ù±‚bc1p# doctest: +SKIPÙ±‚`a
Ù±‚`bpdÙ±‚aoa.Ù±‚`Ù¢„lread_parquetÙ „fpandase1.4.1fmodulexpandas.io.parquet.read_parquetfmoduleõÙ±‚`a(Ù±‚bs1a'Ù±‚bs1odf.parquet.gzipÙ±‚bs1a'Ù±‚`a)Ù±‚`b  Ù±‚bc1p# doctest: +SKIPx)   col1  col2
0     1     3
1     2     4hcompiledÙ¹‚Ù§xšIf you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files.€Ù´ƒ˜Ù±‚bknfimportÙ±‚`a Ù±‚bnnÙ¢„bioÙ „fpandase1.4.1fmoduleipandas.iofmoduleõÙ±‚`a
Ù±‚`afÙ±‚`a Ù±‚aoa=Ù±‚`a Ù±‚`Ù¢„bioÙ „fpandase1.4.1fmoduleipandas.iofmoduleõÙ±‚aoa.Ù±‚`gBytesIOÙ±‚`a(Ù±‚`a)Ù±‚`a
Ù±‚`bdfÙ±‚aoa.Ù±‚`Ù¢„jto_parquetÙ „fpandase1.4.1fmodulex&pandas.core.frame.DataFrame.to_parquetfmoduleõÙ±‚`a(Ù±‚`afÙ±‚`a)Ù±‚`a
Ù±‚`afÙ±‚aoa.Ù±‚`Ù¢„dseekÙ „fpandase1.4.1fmodulexpandas._typing.BaseBuffer.seekfmoduleõÙ±‚`a(Ù±‚bmia0Ù±‚`a)a0hcompiledÙ´ƒ‰Ù±‚`gcontentÙ±‚`a Ù±‚aoa=Ù±‚`a Ù±‚`afÙ±‚aoa.Ù±‚`dreadÙ±‚`a(Ù±‚`a)`hcompiledö„Ù¼ƒÙ»ƒpDataFrame.to_csvööÙ¹‚Ù§qWrite a csv file.€öÙ¼ƒÙ»ƒpDataFrame.to_hdfööÙ¹‚Ù§mWrite to hdf.€öÙ¼ƒÙ»ƒpDataFrame.to_sqlööÙ¹‚Ù§uWrite to a sql table.€öÙ¼ƒÙ»ƒlread_parquetxpandas.io.parquet.read_parquetõÙ¹‚Ù§tRead a parquet file.€öe1.4.1Ù«yto_parquet(self, path: 'FilePath | WriteBuffer[bytes] | None' = None, engine: 'str' = 'auto', compression: 'str | None' = 'snappy', index: 'bool | None' = None, partition_cols: 'list[str] | None' = None, storage_options: 'StorageOptions' = None, **kwargs) -> 'bytes | None'öx&pandas.core.frame.DataFrame.to_parquet€