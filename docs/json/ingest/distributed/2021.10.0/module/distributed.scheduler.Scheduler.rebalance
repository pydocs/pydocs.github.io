Ùª­jAttributesÙ¯‚€öpExtended SummaryÙ¯‚†ÙÆƒgwarning`Ù¹‚Ù§x–This operation is generally not well tested against normal operation of the scheduler. It is not recommended to use it while waiting on computations. €Ù¹‚Ù©Ù§iAlgorithm€ÙÇ‚Ù¹‚ƒÙ§x–Find the mean occupancy of the cluster, defined as data managed by dask +    unmanaged process memory that has been there for at least 30 seconds    (Ù¡x,distributed.worker.memory.recent-to-old-timeÙ§xE).    This lets us ignore temporary spikes caused by task heap usage.€Ù¹‚ƒÙ§x„Alternatively, you may change how memory is measured both for the individual    workers as well as to calculate the mean through    Ù¡x+distributed.worker.memory.rebalance.measureÙ§xO. Namely, this can be useful    to disregard inaccurate OS memory measurements.€ÙÇ†Ù¹‚ƒÙ§xODiscard workers whose occupancy is within 5% of the mean cluster occupancy    (Ù¡x8distributed.worker.memory.rebalance.sender-recipient-gapÙ§xL / 2).    This helps avoid data from bouncing around the cluster repeatedly.€Ù¹‚Ù§x?Workers above the mean are senders; those below are recipients.€Ù¹‚ƒÙ§x:Discard senders whose absolute occupancy is below 30%    (Ù¡x.distributed.worker.memory.rebalance.sender-minÙ§xf). In other words, no data    is moved regardless of imbalancing as long as all workers are below 30%.€Ù¹‚…Ù§x=Discard recipients whose absolute occupancy is above 60%    (Ù¡x1distributed.worker.memory.rebalance.recipient-maxÙ§x<).    Note that this threshold by default is the same as    Ù¡x distributed.worker.memory.targetÙ§xS to prevent workers from accepting data    and immediately spilling it out to disk.€Ù¹‚ƒÙ§xZIteratively pick the sender and recipient that are farthest from the mean and    move the Ù¨Ù§wleast recently insertedÙ§x_ key between the two, until either all    senders or all recipients fall within 5% of the mean.€Ù¹‚Ù§yA recipient will be skipped if it already has a copy of the data. In other    words, this method does not degrade replication.    A key will be skipped if there are no recipients available with enough memory    to accept the key and that don't already hold a copy.€Ù¹‚Ù§xìThe least recently insertd (LRI) policy is a greedy choice with the advantage of being O(1), trivial to implement (it relies on python dict insertion-sorting) and hopefully good enough in most cases. Discarded alternative policies were:€ÙÈ‚Ù¹‚Ù§x°Largest first. O(n*log(n)) save for non-trivial additional data structures and   risks causing the largest chunks of data to repeatedly move around the   cluster like pinballs.€Ù¹‚Ù§yoLeast recently used (LRU). This information is currently available on the   workers only and not trivial to replicate on the scheduler; transmitting it   over the network would be very expensive. Also, note that dask will go out of   its way to minimise the amount of time intermediate keys are held in memory,   so in such a case LRI is a close approximation of LRU.€ögMethodsÙ¯‚€öeNotesÙ¯‚€öpOther ParametersÙ¯‚€öjParametersÙ¯‚‚Ù°ƒnkeys: optional`Ù¹‚Ù§xûwhitelist of dask keys that should be considered for moving. All other keys will be ignored. Note that this offers no guarantee that a key will actually be moved (e.g. because it is unnecessary or because there are no viable recipient workers for it).€Ù°ƒqworkers: optional`Ù¹‚Ù§x¾whitelist of workers addresses to be considered as senders or recipients. All other workers will be ignored. The mean cluster occupancy will be calculated only using the whitelisted workers.€öfRaisesÙ¯‚€öhReceivesÙ¯‚€ögReturnsÙ¯‚€ögSummaryÙ¯‚Ù¹‚Ù§xdRebalance keys so that each worker ends up with roughly the same process memory (managed+unmanaged).€öhWarningsÙ¯‚€öeWarnsÙ¯‚€öfYieldsÙ¯‚€öƒgSummarypExtended SummaryjParametersx/distributed/scheduler.py£r<class 'function'>xdistributed.Scheduler.rebalanceÙ¯‚€ö€i2021.10.0Ù«xfrebalance(self, comm=None, keys: 'Iterable[Hashable]' = None, workers: 'Iterable[str]' = None) -> dictöx)distributed.scheduler.Scheduler.rebalance€